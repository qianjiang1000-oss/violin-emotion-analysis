{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Cell 1: Setup and Installation","metadata":{}},{"cell_type":"code","source":"!pip install librosa scikit-learn matplotlib numpy pandas noisereduce tensorflow shap\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell 1.5: Unzip","metadata":{}},{"cell_type":"code","source":"# Cell X: Upload & Preprocess Audio Data\nimport os\n\n# Path to your uploaded ZIP (replace 'your-data.zip' with your actual file name from Inputs)\nzip_path = '/kaggle/input/your-data.zip'\ndata_dir = '/kaggle/working/violin-emotion-analysis/data'\ncsv_path = '/kaggle/working/violin-emotion-analysis/emotion_labels.csv'\n\n# Unzip the data\n!unzip -q {zip_path} -d {data_dir}\n\n# Check folder structure\nprint(\"Data directory structure after unzipping:\")\nfor root, dirs, files in os.walk(data_dir):\n    level = root.replace(data_dir, '').count(os.sep)\n    indent = ' ' * 2 * level\n    print(f\"{indent}{os.path.basename(root)}/\")\n    subindent = ' ' * 2 * (level + 1)\n    for f in files:\n        print(f\"{subindent}{f}\")\n\n# Check if CSV exists\nif not os.path.exists(csv_path):\n    print(\"\\nWARNING: 'emotion_labels.csv' not found!\")\n    print(\"Please upload your CSV to match filenames and soft emotion labels.\")\nelse:\n    print(f\"\\nCSV file found: {csv_path}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell 2: Imports and Updated Utility Functions","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport librosa\nimport librosa.display\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom sklearn.ensemble import RandomForestClassifier\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization, Attention, Input\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nimport noisereduce as nr\nimport shap\nimport joblib\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Create directories\nos.makedirs('/kaggle/working/violin-emotion-analysis/data', exist_ok=True)\nos.makedirs('/kaggle/working/violin-emotion-analysis/models', exist_ok=True)\n\ndef extract_temporal_features(file_path, sr=44100, frame_length=2048, hop_length=512, window_size=3.0, hop_time=1.5):\n    \"\"\"\n    Extract time-based audio features from overlapping windows (captures emotion dynamics)\n    \"\"\"\n    y, sr = librosa.load(file_path, sr=sr)\n    y = nr.reduce_noise(y=y, sr=sr)\n\n    total_duration = librosa.get_duration(y=y, sr=sr)\n    step = int(hop_time * sr)\n    window = int(window_size * sr)\n\n    feature_sequences = []\n    for start in range(0, len(y) - window, step):\n        segment = y[start:start + window]\n        segment_features = []\n        mfccs = librosa.feature.mfcc(y=segment, sr=sr, n_mfcc=13)\n        chroma = librosa.feature.chroma_stft(y=segment, sr=sr)\n        spectral_contrast = librosa.feature.spectral_contrast(y=segment, sr=sr)\n        rms = librosa.feature.rms(y=segment)\n        zcr = librosa.feature.zero_crossing_rate(y=segment)\n        centroid = librosa.feature.spectral_centroid(y=segment, sr=sr)\n        bandwidth = librosa.feature.spectral_bandwidth(y=segment, sr=sr)\n        \n        # Combine flattened statistics\n        feature_vec = np.hstack([\n            np.mean(mfccs, axis=1),\n            np.std(mfccs, axis=1),\n            np.mean(chroma, axis=1),\n            np.mean(spectral_contrast, axis=1),\n            np.mean(rms),\n            np.mean(zcr),\n            np.mean(centroid),\n            np.mean(bandwidth)\n        ])\n        feature_sequences.append(feature_vec)\n\n    return np.array(feature_sequences)\n\ndef create_temporal_dataset(data_dir, emotion_csv_path):\n    \"\"\"\n    Create dataset with temporal features and soft emotion labels.\n    emotion_csv_path must include columns: filename, emotion, and soft labels like happy, sad, calm, etc.\n    \"\"\"\n    annotations = pd.read_csv(emotion_csv_path)\n    X, y_soft = [], []\n\n    for _, row in annotations.iterrows():\n        file_path = os.path.join(data_dir, row['filename'])\n        if not os.path.exists(file_path): \n            continue\n\n        features = extract_temporal_features(file_path)\n        X.append(features)\n        y_soft.append(row[3:].values.astype(float))  # assuming first 3 columns are meta info\n\n    # Pad sequences for LSTM\n    max_len = max(x.shape[0] for x in X)\n    num_features = X[0].shape[1]\n    X_padded = np.zeros((len(X), max_len, num_features))\n    for i, seq in enumerate(X):\n        X_padded[i, :seq.shape[0], :] = seq\n\n    return np.array(X_padded), np.array(y_soft)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell 3: Data Preparation","metadata":{}},{"cell_type":"code","source":"data_dir = '/kaggle/working/violin-emotion-analysis/data'\nemotion_csv_path = '/kaggle/working/violin-emotion-analysis/emotion_labels.csv'  # <-- upload your CSV here\n\nif not os.path.exists(emotion_csv_path):\n    print(\"Please upload 'emotion_labels.csv' with soft emotion probabilities.\")\nelse:\n    X, y_soft = create_temporal_dataset(data_dir, emotion_csv_path)\n    print(f\"Loaded {X.shape[0]} samples with {X.shape[2]} features each and {y_soft.shape[1]} soft emotion dimensions.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell 4: Model Training (LSTM + RandomForest Fusion)","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import Model\n\n# Split data\nX_train, X_test, y_train_soft, y_test_soft = train_test_split(\n    X, y_soft, test_size=0.2, random_state=42\n)\n\n# Normalize feature values\nscaler = StandardScaler()\nfor i in range(X_train.shape[0]):\n    X_train[i] = scaler.fit_transform(X_train[i])\nfor i in range(X_test.shape[0]):\n    X_test[i] = scaler.transform(X_test[i])\n\njoblib.dump(scaler, '/kaggle/working/violin-emotion-analysis/models/scaler.pkl')\n\n# Define LSTM model with Attention layer\ninput_layer = Input(shape=(X_train.shape[1], X_train.shape[2]))\nx = LSTM(128, return_sequences=True)(input_layer)\nx = Attention()([x, x])\nx = BatchNormalization()(x)\nx = Dropout(0.3)(x)\nx = LSTM(64)(x)\nx = BatchNormalization()(x)\nx = Dropout(0.3)(x)\nx = Dense(64, activation='relu')(x)\noutput_layer = Dense(y_train_soft.shape[1], activation='softmax')(x)\nlstm_model = Model(inputs=input_layer, outputs=output_layer)\n\nlstm_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Callbacks\nearly_stopping = EarlyStopping(patience=10, restore_best_weights=True)\nreduce_lr = ReduceLROnPlateau(factor=0.5, patience=5, min_lr=1e-6)\n\nprint(\"Training LSTM model with temporal and soft label support...\")\nhistory = lstm_model.fit(\n    X_train, y_train_soft,\n    validation_data=(X_test, y_test_soft),\n    epochs=80, batch_size=16,\n    callbacks=[early_stopping, reduce_lr],\n    verbose=1\n)\n\n# Train RandomForest on averaged features (feature-based model)\nX_train_flat = np.mean(X_train, axis=1)\nX_test_flat = np.mean(X_test, axis=1)\nrf_model = RandomForestClassifier(n_estimators=100, random_state=42)\nrf_model.fit(X_train_flat, np.argmax(y_train_soft, axis=1))\n\n# Combine (fusion) predictions\nlstm_probs = lstm_model.predict(X_test)\nrf_probs = np.zeros_like(lstm_probs)\nfor i, pred in enumerate(rf_model.predict_proba(X_test_flat)):\n    rf_probs[i, :len(pred)] = pred\n\nhybrid_probs = 0.6 * lstm_probs + 0.4 * rf_probs\nhybrid_pred = np.argmax(hybrid_probs, axis=1)\ntrue_labels = np.argmax(y_test_soft, axis=1)\n\naccuracy = accuracy_score(true_labels, hybrid_pred)\nprint(f\"Hybrid Model Accuracy: {accuracy:.4f}\")\n\nlstm_model.save('/kaggle/working/violin-emotion-analysis/models/lstm_hybrid_model.h5')\njoblib.dump(rf_model, '/kaggle/working/violin-emotion-analysis/models/rf_model.pkl')\nprint(\"Models saved successfully!\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell 5: Explainability and Visualization","metadata":{}},{"cell_type":"code","source":"explainer = shap.Explainer(rf_model, X_train_flat[:100])\nshap_values = explainer(X_test_flat[:50])\n\nprint(\"Generating SHAP summary plot (Random Forest interpretability)...\")\nshap.summary_plot(shap_values, X_test_flat[:50], show=True)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell 6: Real-Time Prediction Simulation on New Audio Files","metadata":{}},{"cell_type":"code","source":"new_audio_dir = '/kaggle/working/violin-emotion-analysis/new_audio'  # put new audios here\nemotion_labels = pd.read_csv(emotion_csv_path).columns[3:].tolist()  # same soft labels as training\n\nnew_files = [f for f in os.listdir(new_audio_dir) if f.endswith('.wav')]\nprint(f\"Found {len(new_files)} new audio files for prediction.\")\n\nfor f in new_files:\n    path = os.path.join(new_audio_dir, f)\n    \n    # Extract features\n    features = extract_temporal_features(path)\n    \n    # Pad sequence to match LSTM input\n    padded = np.zeros((1, X_train.shape[1], X_train.shape[2]))\n    padded[0, :features.shape[0], :] = features\n    \n    # LSTM prediction\n    lstm_probs = lstm_model.predict(padded)\n    \n    # RF prediction\n    rf_probs = rf_model.predict_proba(np.mean(padded, axis=1))\n    rf_probs = np.array([np.pad(p, (0, len(emotion_labels) - len(p))) for p in rf_probs])\n    \n    # Hybrid fusion\n    hybrid_probs = 0.6*lstm_probs + 0.4*rf_probs\n    \n    # Perceptual smoothing\n    smoothed = np.convolve(np.mean(hybrid_probs, axis=0), np.ones(3)/3, mode='same')\n    \n    # Plot results\n    plt.figure(figsize=(8, 5))\n    plt.bar(emotion_labels, smoothed)\n    plt.title(f\"Predicted Emotions for {f}\")\n    plt.ylabel(\"Probability\")\n    plt.ylim(0,1)\n    plt.show()\n    \n    # Print prediction\n    pred_emotion = emotion_labels[np.argmax(smoothed)]\n    print(f\"Audio: {f} → Predicted Emotion: {pred_emotion}\")\n    print(\"Probability per emotion:\")\n    for label, prob in zip(emotion_labels, smoothed):\n        print(f\"  {label}: {prob:.3f}\")\n    print(\"\\n\" + \"-\"*40 + \"\\n\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}