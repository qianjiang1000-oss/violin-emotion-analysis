{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13470958,"sourceType":"datasetVersion","datasetId":8551421}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Cell 1: Setup and Installation","metadata":{}},{"cell_type":"code","source":"!pip install librosa scikit-learn matplotlib numpy pandas noisereduce tensorflow shap","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-23T00:08:41.793416Z","iopub.execute_input":"2025-10-23T00:08:41.793736Z","iopub.status.idle":"2025-10-23T00:08:47.116004Z","shell.execute_reply.started":"2025-10-23T00:08:41.793716Z","shell.execute_reply":"2025-10-23T00:08:47.115248Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (0.11.0)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.2.2)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.7.2)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.3)\nCollecting noisereduce\n  Downloading noisereduce-3.0.3-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\nRequirement already satisfied: shap in /usr/local/lib/python3.11/dist-packages (0.44.1)\nRequirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa) (3.0.1)\nRequirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.60.0)\nRequirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.15.3)\nRequirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.5.2)\nRequirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.4.2)\nRequirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.13.1)\nRequirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.8.2)\nRequirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.5.0.post1)\nRequirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.15.0)\nRequirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.4)\nRequirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.1.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.59.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (25.0)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.3.0)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy) (2.4.1)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from noisereduce) (4.67.1)\nRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\nRequirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\nRequirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\nRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\nRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.20.3)\nRequirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.5)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\nRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\nRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\nRequirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.75.1)\nRequirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\nRequirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\nRequirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.14.0)\nRequirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\nRequirement already satisfied: slicer==0.0.7 in /usr/local/lib/python3.11/dist-packages (from shap) (0.0.7)\nRequirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from shap) (3.1.1)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\nRequirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (14.1.0)\nRequirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\nRequirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.16.0)\nRequirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\nRequirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (4.4.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.3)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.8.3)\nRequirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->librosa) (2.0.0)\nRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8.2)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy) (2024.2.0)\nRequirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.23)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy) (2024.2.0)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\nDownloading noisereduce-3.0.3-py3-none-any.whl (22 kB)\nInstalling collected packages: noisereduce\nSuccessfully installed noisereduce-3.0.3\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Cell 1.5","metadata":{}},{"cell_type":"code","source":"# Cell 1.5: Updated - No unzip needed since files are already extracted\nimport os\nimport shutil\n\n# Paths to your uploaded files (they're already extracted)\ndataset_path = '/kaggle/input/training-audio'\ndata_dir = '/kaggle/working/violin-emotion-analysis/data'\ncsv_path = '/kaggle/input/training-audio/emotion_labels.csv'\n\n# Create the working directory\nos.makedirs(data_dir, exist_ok=True)\n\n# Copy all audio files from the dataset to our working directory\naudio_source_dir = '/kaggle/input/training-audio/audio'\nif os.path.exists(audio_source_dir):\n    # Copy all WAV files to our working directory\n    for file_name in os.listdir(audio_source_dir):\n        if file_name.endswith('.wav'):\n            source_path = os.path.join(audio_source_dir, file_name)\n            dest_path = os.path.join(data_dir, file_name)\n            shutil.copy2(source_path, dest_path)\n    \n    print(f\"✅ Copied audio files to working directory\")\nelse:\n    print(\"❌ Audio folder not found!\")\n\n# Check folder structure\nprint(\"\\nData directory structure:\")\nfor root, dirs, files in os.walk(data_dir):\n    level = root.replace(data_dir, '').count(os.sep)\n    indent = ' ' * 2 * level\n    print(f\"{indent}{os.path.basename(root)}/\")\n    subindent = ' ' * 2 * (level + 1)\n    for f in files:\n        print(f\"{subindent}{f}\")\n\n# Check if CSV exists\nif not os.path.exists(csv_path):\n    print(\"\\n❌ WARNING: 'emotion_labels.csv' not found!\")\nelse:\n    print(f\"\\n✅ CSV file found: {csv_path}\")\n    # Also copy CSV to working directory for consistency\n    shutil.copy2(csv_path, '/kaggle/working/violin-emotion-analysis/emotion_labels.csv')\n    print(\"✅ CSV copied to working directory\")\n\nprint(f\"\\n📊 Total audio files ready for processing: {len([f for f in os.listdir(data_dir) if f.endswith('.wav')])}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T00:33:30.767570Z","iopub.execute_input":"2025-10-23T00:33:30.768224Z","iopub.status.idle":"2025-10-23T00:33:30.951729Z","shell.execute_reply.started":"2025-10-23T00:33:30.768199Z","shell.execute_reply":"2025-10-23T00:33:30.951029Z"}},"outputs":[{"name":"stdout","text":"✅ Copied audio files to working directory\n\nData directory structure:\ndata/\n  a9.wav\n  a1.wav\n  a0.wav\n  h0.wav\n  h1.wav\n  a2.wav\n  s1.wav\n  h2.wav\n  s9.wav\n  a6.wav\n  s0.wav\n  s3.wav\n  a4.wav\n  a8.wav\n  h5.wav\n  s7.wav\n  a7.wav\n  h7.wav\n  h8.wav\n  s2.wav\n  a5.wav\n  h6.wav\n  s6.wav\n  s5.wav\n  s8.wav\n  h9.wav\n  a3.wav\n  h3.wav\n  s4.wav\n  h4.wav\n\n✅ CSV file found: /kaggle/input/training-audio/emotion_labels.csv\n✅ CSV copied to working directory\n\n📊 Total audio files ready for processing: 30\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"# Cell 2: Imports and Updated Utility Functions","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport librosa\nimport librosa.display\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom sklearn.ensemble import RandomForestClassifier\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization, Attention, Input\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nimport noisereduce as nr\nimport shap\nimport joblib\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Create directories\nos.makedirs('/kaggle/working/violin-emotion-analysis/data', exist_ok=True)\nos.makedirs('/kaggle/working/violin-emotion-analysis/models', exist_ok=True)\n\ndef extract_temporal_features(file_path, sr=44100, frame_length=2048, hop_length=512, window_size=3.0, hop_time=1.5):\n    \"\"\"\n    Extract time-based audio features from overlapping windows (captures emotion dynamics)\n    \"\"\"\n    y, sr = librosa.load(file_path, sr=sr)\n    y = nr.reduce_noise(y=y, sr=sr)\n\n    total_duration = librosa.get_duration(y=y, sr=sr)\n    step = int(hop_time * sr)\n    window = int(window_size * sr)\n\n    feature_sequences = []\n    for start in range(0, len(y) - window, step):\n        segment = y[start:start + window]\n        segment_features = []\n        mfccs = librosa.feature.mfcc(y=segment, sr=sr, n_mfcc=13)\n        chroma = librosa.feature.chroma_stft(y=segment, sr=sr)\n        spectral_contrast = librosa.feature.spectral_contrast(y=segment, sr=sr)\n        rms = librosa.feature.rms(y=segment)\n        zcr = librosa.feature.zero_crossing_rate(y=segment)\n        centroid = librosa.feature.spectral_centroid(y=segment, sr=sr)\n        bandwidth = librosa.feature.spectral_bandwidth(y=segment, sr=sr)\n        \n        # Combine flattened statistics\n        feature_vec = np.hstack([\n            np.mean(mfccs, axis=1),\n            np.std(mfccs, axis=1),\n            np.mean(chroma, axis=1),\n            np.mean(spectral_contrast, axis=1),\n            np.mean(rms),\n            np.mean(zcr),\n            np.mean(centroid),\n            np.mean(bandwidth)\n        ])\n        feature_sequences.append(feature_vec)\n\n    return np.array(feature_sequences)\n\ndef create_temporal_dataset(data_dir, emotion_csv_path):\n    \"\"\"\n    Create dataset with temporal features and soft emotion labels.\n    emotion_csv_path must include columns: filename, emotion, and soft labels like happy, sad, calm, etc.\n    \"\"\"\n    annotations = pd.read_csv(emotion_csv_path)\n    X, y_soft = [], []\n\n    for _, row in annotations.iterrows():\n        file_path = os.path.join(data_dir, row['filename'])\n        if not os.path.exists(file_path): \n            continue\n\n        features = extract_temporal_features(file_path)\n        X.append(features)\n        y_soft.append(row[4:].values.astype(float))  # ← NEW (gives 3 dimensions)\n        \n    # Pad sequences for LSTM\n    max_len = max(x.shape[0] for x in X)\n    num_features = X[0].shape[1]\n    X_padded = np.zeros((len(X), max_len, num_features))\n    for i, seq in enumerate(X):\n        X_padded[i, :seq.shape[0], :] = seq\n\n    return np.array(X_padded), np.array(y_soft)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T00:33:46.305993Z","iopub.execute_input":"2025-10-23T00:33:46.306549Z","iopub.status.idle":"2025-10-23T00:33:46.318140Z","shell.execute_reply.started":"2025-10-23T00:33:46.306528Z","shell.execute_reply":"2025-10-23T00:33:46.317408Z"}},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":"# Cell 3: Data Preparation","metadata":{}},{"cell_type":"code","source":"data_dir = '/kaggle/working/violin-emotion-analysis/data'\nemotion_csv_path = '/kaggle/working/violin-emotion-analysis/emotion_labels.csv'  # <-- upload your CSV here\n\nif not os.path.exists(emotion_csv_path):\n    print(\"Please upload 'emotion_labels.csv' with soft emotion probabilities.\")\nelse:\n    X, y_soft = create_temporal_dataset(data_dir, emotion_csv_path)\n    print(f\"Loaded {X.shape[0]} samples with {X.shape[2]} features each and {y_soft.shape[1]} soft emotion dimensions.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T00:33:51.301036Z","iopub.execute_input":"2025-10-23T00:33:51.301690Z","iopub.status.idle":"2025-10-23T00:34:32.269395Z","shell.execute_reply.started":"2025-10-23T00:33:51.301669Z","shell.execute_reply":"2025-10-23T00:34:32.268459Z"}},"outputs":[{"name":"stdout","text":"Loaded 30 samples with 49 features each and 3 soft emotion dimensions.\n","output_type":"stream"}],"execution_count":22},{"cell_type":"markdown","source":"# Cell 4: Model Training (LSTM + Random Forest)","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import Model\n\n# Split data\nX_train, X_test, y_train_soft, y_test_soft = train_test_split(\n    X, y_soft, test_size=0.2, random_state=42\n)\n\n# Normalize feature values\nscaler = StandardScaler()\nfor i in range(X_train.shape[0]):\n    X_train[i] = scaler.fit_transform(X_train[i])\nfor i in range(X_test.shape[0]):\n    X_test[i] = scaler.transform(X_test[i])\n\njoblib.dump(scaler, '/kaggle/working/violin-emotion-analysis/models/scaler.pkl')\n\n# Define LSTM model with Attention layer\ninput_layer = Input(shape=(X_train.shape[1], X_train.shape[2]))\nx = LSTM(128, return_sequences=True)(input_layer)\nx = Attention()([x, x])\nx = BatchNormalization()(x)\nx = Dropout(0.3)(x)\nx = LSTM(64)(x)\nx = BatchNormalization()(x)\nx = Dropout(0.3)(x)\nx = Dense(64, activation='relu')(x)\noutput_layer = Dense(y_train_soft.shape[1], activation='softmax')(x)\nlstm_model = Model(inputs=input_layer, outputs=output_layer)\n\nlstm_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Callbacks\nearly_stopping = EarlyStopping(patience=10, restore_best_weights=True)\nreduce_lr = ReduceLROnPlateau(factor=0.5, patience=5, min_lr=1e-6)\n\nprint(\"Training LSTM model with temporal and soft label support...\")\nhistory = lstm_model.fit(\n    X_train, y_train_soft,\n    validation_data=(X_test, y_test_soft),\n    epochs=80, batch_size=16,\n    callbacks=[early_stopping, reduce_lr],\n    verbose=1\n)\n\n# Train RandomForest on averaged features (feature-based model)\nX_train_flat = np.mean(X_train, axis=1)\nX_test_flat = np.mean(X_test, axis=1)\nrf_model = RandomForestClassifier(n_estimators=100, random_state=42)\nrf_model.fit(X_train_flat, np.argmax(y_train_soft, axis=1))\n\n# Combine (fusion) predictions\nlstm_probs = lstm_model.predict(X_test)\nrf_probs = np.zeros_like(lstm_probs)\nfor i, pred in enumerate(rf_model.predict_proba(X_test_flat)):\n    rf_probs[i, :len(pred)] = pred\n\nhybrid_probs = 0.6 * lstm_probs + 0.4 * rf_probs\nhybrid_pred = np.argmax(hybrid_probs, axis=1)\ntrue_labels = np.argmax(y_test_soft, axis=1)\n\naccuracy = accuracy_score(true_labels, hybrid_pred)\nprint(f\"Hybrid Model Accuracy: {accuracy:.4f}\")\n\nlstm_model.save('/kaggle/working/violin-emotion-analysis/models/lstm_hybrid_model.h5')\njoblib.dump(rf_model, '/kaggle/working/violin-emotion-analysis/models/rf_model.pkl')\nprint(\"Models saved successfully!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T00:35:48.761601Z","iopub.execute_input":"2025-10-23T00:35:48.762330Z","iopub.status.idle":"2025-10-23T00:35:55.506938Z","shell.execute_reply.started":"2025-10-23T00:35:48.762299Z","shell.execute_reply":"2025-10-23T00:35:55.506162Z"}},"outputs":[{"name":"stdout","text":"Training LSTM model with temporal and soft label support...\nEpoch 1/80\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 623ms/step - accuracy: 0.2292 - loss: 1.8342 - val_accuracy: 0.0000e+00 - val_loss: 1.2325 - learning_rate: 0.0010\nEpoch 2/80\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.4028 - loss: 1.3173 - val_accuracy: 0.0000e+00 - val_loss: 1.2134 - learning_rate: 0.0010\nEpoch 3/80\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.4444 - loss: 1.5235 - val_accuracy: 0.1667 - val_loss: 1.2417 - learning_rate: 0.0010\nEpoch 4/80\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.1736 - loss: 1.3264 - val_accuracy: 0.1667 - val_loss: 1.2440 - learning_rate: 0.0010\nEpoch 5/80\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.4167 - loss: 1.1270 - val_accuracy: 0.3333 - val_loss: 1.2458 - learning_rate: 0.0010\nEpoch 6/80\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.6042 - loss: 0.8972 - val_accuracy: 0.3333 - val_loss: 1.2665 - learning_rate: 0.0010\nEpoch 7/80\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.4028 - loss: 1.1532 - val_accuracy: 0.3333 - val_loss: 1.2816 - learning_rate: 0.0010\nEpoch 8/80\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.3542 - loss: 1.2164 - val_accuracy: 0.1667 - val_loss: 1.2864 - learning_rate: 5.0000e-04\nEpoch 9/80\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.3958 - loss: 1.0658 - val_accuracy: 0.1667 - val_loss: 1.2887 - learning_rate: 5.0000e-04\nEpoch 10/80\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.7500 - loss: 0.8844 - val_accuracy: 0.1667 - val_loss: 1.2899 - learning_rate: 5.0000e-04\nEpoch 11/80\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.5972 - loss: 0.9337 - val_accuracy: 0.1667 - val_loss: 1.2906 - learning_rate: 5.0000e-04\nEpoch 12/80\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.5764 - loss: 0.8377 - val_accuracy: 0.1667 - val_loss: 1.2926 - learning_rate: 5.0000e-04\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 269ms/step\nHybrid Model Accuracy: 0.0000\nModels saved successfully!\n","output_type":"stream"}],"execution_count":23},{"cell_type":"markdown","source":"# Cell 5: Explainability and Visualization","metadata":{}},{"cell_type":"code","source":"explainer = shap.Explainer(rf_model, X_train_flat[:100])\nshap_values = explainer(X_test_flat[:50])\n\nprint(\"Generating SHAP summary plot (Random Forest interpretability)...\")\nshap.summary_plot(shap_values, X_test_flat[:50], show=True)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell 6: Real-Time Prediction Simulation on New Audio Files","metadata":{}},{"cell_type":"code","source":"new_audio_dir = '/kaggle/working/violin-emotion-analysis/new_audio'  # put new audios here\nemotion_labels = pd.read_csv(emotion_csv_path).columns[3:].tolist()  # same soft labels as training\n\nnew_files = [f for f in os.listdir(new_audio_dir) if f.endswith('.wav')]\nprint(f\"Found {len(new_files)} new audio files for prediction.\")\n\nfor f in new_files:\n    path = os.path.join(new_audio_dir, f)\n    \n    # Extract features\n    features = extract_temporal_features(path)\n    \n    # Pad sequence to match LSTM input\n    padded = np.zeros((1, X_train.shape[1], X_train.shape[2]))\n    padded[0, :features.shape[0], :] = features\n    \n    # LSTM prediction\n    lstm_probs = lstm_model.predict(padded)\n    \n    # RF prediction\n    rf_probs = rf_model.predict_proba(np.mean(padded, axis=1))\n    rf_probs = np.array([np.pad(p, (0, len(emotion_labels) - len(p))) for p in rf_probs])\n    \n    # Hybrid fusion\n    hybrid_probs = 0.6*lstm_probs + 0.4*rf_probs\n    \n    # Perceptual smoothing\n    smoothed = np.convolve(np.mean(hybrid_probs, axis=0), np.ones(3)/3, mode='same')\n    \n    # Plot results\n    plt.figure(figsize=(8, 5))\n    plt.bar(emotion_labels, smoothed)\n    plt.title(f\"Predicted Emotions for {f}\")\n    plt.ylabel(\"Probability\")\n    plt.ylim(0,1)\n    plt.show()\n    \n    # Print prediction\n    pred_emotion = emotion_labels[np.argmax(smoothed)]\n    print(f\"Audio: {f} → Predicted Emotion: {pred_emotion}\")\n    print(\"Probability per emotion:\")\n    for label, prob in zip(emotion_labels, smoothed):\n        print(f\"  {label}: {prob:.3f}\")\n    print(\"\\n\" + \"-\"*40 + \"\\n\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}