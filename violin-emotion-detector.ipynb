{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13477706,"sourceType":"datasetVersion","datasetId":8556482}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Cell 1: Setup and Installation","metadata":{}},{"cell_type":"code","source":"!pip install librosa scikit-learn matplotlib numpy pandas noisereduce tensorflow shap","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell 1.5","metadata":{}},{"cell_type":"code","source":"# Cell 1.5: Updated - No unzip needed since files are already extracted\nimport os\nimport shutil\n\n# Paths to your uploaded files (they're already extracted)\ndataset_path = '/kaggle/input/train-audio'\ndata_dir = '/kaggle/working/violin-emotion-analysis/data'\ncsv_path = '/kaggle/input/train-audio/emotion_labels.csv'\n\n# Create the working directory\nos.makedirs(data_dir, exist_ok=True)\n\n# Copy all audio files from the dataset to our working directory\naudio_source_dir = '/kaggle/input/train-audio/audio'\nif os.path.exists(audio_source_dir):\n    # Copy all WAV files to our working directory\n    for file_name in os.listdir(audio_source_dir):\n        if file_name.endswith('.wav'):\n            source_path = os.path.join(audio_source_dir, file_name)\n            dest_path = os.path.join(data_dir, file_name)\n            shutil.copy2(source_path, dest_path)\n    \n    print(f\"‚úÖ Copied audio files to working directory\")\nelse:\n    print(\"‚ùå Audio folder not found!\")\n\n# Check folder structure\nprint(\"\\nData directory structure:\")\nfor root, dirs, files in os.walk(data_dir):\n    level = root.replace(data_dir, '').count(os.sep)\n    indent = ' ' * 2 * level\n    print(f\"{indent}{os.path.basename(root)}/\")\n    subindent = ' ' * 2 * (level + 1)\n    for f in files:\n        print(f\"{subindent}{f}\")\n\n# Check if CSV exists\nif not os.path.exists(csv_path):\n    print(\"\\n‚ùå WARNING: 'emotion_labels.csv' not found!\")\nelse:\n    print(f\"\\n‚úÖ CSV file found: {csv_path}\")\n    # Also copy CSV to working directory for consistency\n    shutil.copy2(csv_path, '/kaggle/working/violin-emotion-analysis/emotion_labels.csv')\n    print(\"‚úÖ CSV copied to working directory\")\n\nprint(f\"\\nüìä Total audio files ready for processing: {len([f for f in os.listdir(data_dir) if f.endswith('.wav')])}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell 2: Imports and Updated Utility Functions","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport librosa\nimport librosa.display\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom sklearn.ensemble import RandomForestClassifier\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization, Attention, Input\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nimport noisereduce as nr\nimport shap\nimport joblib\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Create directories\nos.makedirs('/kaggle/working/violin-emotion-analysis/data', exist_ok=True)\nos.makedirs('/kaggle/working/violin-emotion-analysis/models', exist_ok=True)\n\ndef extract_temporal_features(file_path, sr=44100, frame_length=2048, hop_length=512, window_size=3.0, hop_time=1.5):\n    \"\"\"\n    Extract time-based audio features from overlapping windows (captures emotion dynamics)\n    \"\"\"\n    y, sr = librosa.load(file_path, sr=sr)\n    y = nr.reduce_noise(y=y, sr=sr)\n\n    total_duration = librosa.get_duration(y=y, sr=sr)\n    step = int(hop_time * sr)\n    window = int(window_size * sr)\n\n    feature_sequences = []\n    for start in range(0, len(y) - window, step):\n        segment = y[start:start + window]\n        segment_features = []\n        mfccs = librosa.feature.mfcc(y=segment, sr=sr, n_mfcc=13)\n        chroma = librosa.feature.chroma_stft(y=segment, sr=sr)\n        spectral_contrast = librosa.feature.spectral_contrast(y=segment, sr=sr)\n        rms = librosa.feature.rms(y=segment)\n        zcr = librosa.feature.zero_crossing_rate(y=segment)\n        centroid = librosa.feature.spectral_centroid(y=segment, sr=sr)\n        bandwidth = librosa.feature.spectral_bandwidth(y=segment, sr=sr)\n        \n        # Combine flattened statistics\n        feature_vec = np.hstack([\n            np.mean(mfccs, axis=1),\n            np.std(mfccs, axis=1),\n            np.mean(chroma, axis=1),\n            np.mean(spectral_contrast, axis=1),\n            np.mean(rms),\n            np.mean(zcr),\n            np.mean(centroid),\n            np.mean(bandwidth)\n        ])\n        feature_sequences.append(feature_vec)\n\n    return np.array(feature_sequences)\n\ndef create_temporal_dataset(data_dir, emotion_csv_path):\n    \"\"\"\n    Create dataset with temporal features and soft emotion labels.\n    emotion_csv_path must include columns: filename, emotion, and soft labels like happy, sad, calm, etc.\n    \"\"\"\n    annotations = pd.read_csv(emotion_csv_path)\n    X, y_soft = [], []\n\n    for _, row in annotations.iterrows():\n        file_path = os.path.join(data_dir, row['filename'])\n        if not os.path.exists(file_path): \n            continue\n\n        features = extract_temporal_features(file_path)\n        X.append(features)\n        y_soft.append(row[4:].values.astype(float))  # ‚Üê NEW (gives 3 dimensions)\n        \n    # Pad sequences for LSTM\n    max_len = max(x.shape[0] for x in X)\n    num_features = X[0].shape[1]\n    X_padded = np.zeros((len(X), max_len, num_features))\n    for i, seq in enumerate(X):\n        X_padded[i, :seq.shape[0], :] = seq\n\n    return np.array(X_padded), np.array(y_soft)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell 3: Data Preparation","metadata":{}},{"cell_type":"code","source":"data_dir = '/kaggle/working/violin-emotion-analysis/data'\nemotion_csv_path = '/kaggle/working/violin-emotion-analysis/emotion_labels.csv'  # <-- upload your CSV here\n\nif not os.path.exists(emotion_csv_path):\n    print(\"Please upload 'emotion_labels.csv' with soft emotion probabilities.\")\nelse:\n    X, y_soft = create_temporal_dataset(data_dir, emotion_csv_path)\n    print(f\"Loaded {X.shape[0]} samples with {X.shape[2]} features each and {y_soft.shape[1]} soft emotion dimensions.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell 4: Model Training (LSTM + Random Forest)","metadata":{}},{"cell_type":"code","source":"# Cell 4: Model Training (LSTM + RandomForest) - FIXED VERSION\nfrom tensorflow.keras.models import Model\n\n# Split data\nX_train, X_test, y_train_soft, y_test_soft = train_test_split(\n    X, y_soft, test_size=0.2, random_state=42\n)\n\nprint(f\"Training set: {X_train.shape[0]} samples\")\nprint(f\"Test set: {X_test.shape[0]} samples\")\nprint(f\"Emotion dimensions: {y_train_soft.shape[1]}\")\n\n# Normalize feature values\nscaler = StandardScaler()\nfor i in range(X_train.shape[0]):\n    X_train[i] = scaler.fit_transform(X_train[i])\nfor i in range(X_test.shape[0]):\n    X_test[i] = scaler.transform(X_test[i])\n\njoblib.dump(scaler, '/kaggle/working/violin-emotion-analysis/models/scaler.pkl')\n\n# Define LSTM model with Attention layer\ninput_layer = Input(shape=(X_train.shape[1], X_train.shape[2]))\nx = LSTM(128, return_sequences=True)(input_layer)\nx = Attention()([x, x])\nx = BatchNormalization()(x)\nx = Dropout(0.3)(x)\nx = LSTM(64)(x)\nx = BatchNormalization()(x)\nx = Dropout(0.3)(x)\nx = Dense(64, activation='relu')(x)\noutput_layer = Dense(y_train_soft.shape[1], activation='softmax')(x)\nlstm_model = Model(inputs=input_layer, outputs=output_layer)\n\nlstm_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Callbacks\nearly_stopping = EarlyStopping(patience=10, restore_best_weights=True)\nreduce_lr = ReduceLROnPlateau(factor=0.5, patience=5, min_lr=1e-6)\n\nprint(\"Training LSTM model with temporal and soft label support...\")\nhistory = lstm_model.fit(\n    X_train, y_train_soft,\n    validation_data=(X_test, y_test_soft),\n    epochs=80, batch_size=16,\n    callbacks=[early_stopping, reduce_lr],\n    verbose=1\n)\n\n# Train RandomForest on averaged features (feature-based model)\nX_train_flat = np.mean(X_train, axis=1)\nX_test_flat = np.mean(X_test, axis=1)\nrf_model = RandomForestClassifier(n_estimators=100, random_state=42)\nrf_model.fit(X_train_flat, np.argmax(y_train_soft, axis=1))\n\n# FIXED PREDICTION SECTION WITH DEBUG INFO\nprint(\"\\n\" + \"=\"*50)\nprint(\"PREDICTION DEBUG INFO:\")\nprint(\"=\"*50)\n\n# LSTM predictions\nlstm_probs = lstm_model.predict(X_test)\nprint(f\"LSTM probabilities shape: {lstm_probs.shape}\")\nprint(f\"LSTM probs sample: {lstm_probs[0]}\")\n\n# Random Forest predictions\nrf_probs = rf_model.predict_proba(X_test_flat)\nprint(f\"RF probabilities shape: {rf_probs.shape}\")\nprint(f\"RF probs sample: {rf_probs[0]}\")\n\n# Ensure both probability arrays have the same dimensions\nif rf_probs.shape[1] < lstm_probs.shape[1]:\n    # Pad RF probabilities if needed\n    rf_probs_padded = np.zeros((rf_probs.shape[0], lstm_probs.shape[1]))\n    rf_probs_padded[:, :rf_probs.shape[1]] = rf_probs\n    rf_probs = rf_probs_padded\n    print(f\"Padded RF probabilities to: {rf_probs.shape}\")\n\n# Hybrid fusion\nhybrid_probs = 0.6 * lstm_probs + 0.4 * rf_probs\nprint(f\"Hybrid probs shape: {hybrid_probs.shape}\")\nprint(f\"Hybrid probs sample: {hybrid_probs[0]}\")\n\n# Get predictions\nhybrid_pred = np.argmax(hybrid_probs, axis=1)\ntrue_labels = np.argmax(y_test_soft, axis=1)\n\nprint(f\"True labels: {true_labels}\")\nprint(f\"Predicted labels: {hybrid_pred}\")\n\n# Calculate accuracies\naccuracy = accuracy_score(true_labels, hybrid_pred)\n\n# Individual model accuracies for comparison\nlstm_pred = np.argmax(lstm_probs, axis=1)\nrf_pred = rf_model.predict(X_test_flat)\n\nlstm_accuracy = accuracy_score(true_labels, lstm_pred)\nrf_accuracy = accuracy_score(true_labels, rf_pred)\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"RESULTS:\")\nprint(\"=\"*50)\nprint(f\"LSTM Model Accuracy: {lstm_accuracy:.4f}\")\nprint(f\"Random Forest Accuracy: {rf_accuracy:.4f}\")\nprint(f\"Hybrid Model Accuracy: {accuracy:.4f}\")\n\n# Detailed classification report\nprint(\"\\nDetailed Classification Report (Hybrid Model):\")\nprint(classification_report(true_labels, hybrid_pred, target_names=['Happy', 'Sad', 'Angry']))\n\n# Save models\nlstm_model.save('/kaggle/working/violin-emotion-analysis/models/lstm_hybrid_model.h5')\njoblib.dump(rf_model, '/kaggle/working/violin-emotion-analysis/models/rf_model.pkl')\nprint(\"\\n‚úÖ Models saved successfully!\")\n\n# Plot training history\nplt.figure(figsize=(12, 4))\nplt.subplot(1, 2, 1)\nplt.plot(history.history['loss'], label='Training Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.title('Model Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(history.history['accuracy'], label='Training Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.title('Model Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend()\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell 5: Explainability and Visualization","metadata":{}},{"cell_type":"code","source":"# Cell 5: Enhanced SHAP Analysis with Emotion-Specific Features\nprint(\"üéµ SHAP Analysis - Feature Importance by Emotion\")\n\n# Get emotion labels from your CSV\nemotion_labels = ['Happy', 'Sad', 'Angry']\n\n# Create SHAP explainer\nexplainer = shap.Explainer(rf_model, X_train_flat[:100])  # Use first 100 training samples\nshap_values = explainer(X_test_flat[:50])\n\nprint(\"üîç Analyzing feature importance for each emotion...\")\n\n# Create separate plots for each emotion\nfor emotion_idx, emotion_name in enumerate(emotion_labels):\n    print(f\"\\nüéª {emotion_name.upper()} EMOTION:\")\n    print(\"=\"*40)\n    \n    # Get SHAP values for this specific emotion\n    emotion_shap_values = shap_values.values[:, :, emotion_idx]\n    \n    # Get top 10 most important features for this emotion\n    feature_importance = np.mean(np.abs(emotion_shap_values), axis=0)\n    top_feature_indices = np.argsort(feature_importance)[-10:][::-1]  # Top 10\n    \n    print(f\"Top features that predict {emotion_name}:\")\n    for i, idx in enumerate(top_feature_indices):\n        importance = feature_importance[idx]\n        print(f\"  {i+1}. Feature {idx}: {importance:.4f}\")\n    \n    # Create feature names based on what we know about the extraction\n    feature_categories = []\n    for i in range(49):  # You have 49 features\n        if i < 26:  # First 26 are MFCCs (13 mean + 13 std)\n            if i < 13:\n                feature_categories.append(f\"MFCC_{i+1}_mean\")\n            else:\n                feature_categories.append(f\"MFCC_{i-12}_std\")\n        elif i < 38:  # Next 12 are Chroma (mean only)\n            feature_categories.append(f\"Chroma_{i-25}\")\n        elif i < 45:  # Next 7 are Spectral Contrast\n            feature_categories.append(f\"Spectral_Contrast_{i-37}\")\n        elif i == 45:\n            feature_categories.append(\"RMS_energy\")\n        elif i == 46:\n            feature_categories.append(\"Zero_Crossing_Rate\")\n        elif i == 47:\n            feature_categories.append(\"Spectral_Centroid\")\n        elif i == 48:\n            feature_categories.append(\"Spectral_Bandwidth\")\n        else:\n            feature_categories.append(f\"Feature_{i}\")\n    \n    print(f\"\\nüìä Named features for {emotion_name}:\")\n    for i, idx in enumerate(top_feature_indices[:5]):  # Top 5 with names\n        feature_name = feature_categories[idx]\n        importance = feature_importance[idx]\n        print(f\"  {i+1}. {feature_name}: {importance:.4f}\")\n\n# Create the main SHAP summary plot\nprint(\"\\nüìà Generating main SHAP summary plot...\")\nshap.summary_plot(shap_values, X_test_flat[:50], feature_names=feature_categories, show=False)\nplt.title(\"SHAP Feature Importance - All Emotions\")\nplt.tight_layout()\nplt.show()\n\n# Additional: Force plot for a single prediction to see detailed reasoning\nprint(\"\\nüî¨ Detailed prediction breakdown for first test sample:\")\nshap.force_plot(explainer.expected_value[0], shap_values.values[0,:,0], X_test_flat[0,:], \n                feature_names=feature_categories, matplotlib=True, show=False)\nplt.title(f\"Prediction Breakdown: {emotion_labels[np.argmax(y_test_soft[0])]} ‚Üí {emotion_labels[rf_model.predict(X_test_flat[0:1])[0]]}\")\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell 6: Real-Time Prediction Simulation on New Audio Files","metadata":{}},{"cell_type":"code","source":"new_audio_dir = '/kaggle/working/violin-emotion-analysis/new_audio'  # put new audios here\nemotion_labels = pd.read_csv(emotion_csv_path).columns[3:].tolist()  # same soft labels as training\n\nnew_files = [f for f in os.listdir(new_audio_dir) if f.endswith('.wav')]\nprint(f\"Found {len(new_files)} new audio files for prediction.\")\n\nfor f in new_files:\n    path = os.path.join(new_audio_dir, f)\n    \n    # Extract features\n    features = extract_temporal_features(path)\n    \n    # Pad sequence to match LSTM input\n    padded = np.zeros((1, X_train.shape[1], X_train.shape[2]))\n    padded[0, :features.shape[0], :] = features\n    \n    # LSTM prediction\n    lstm_probs = lstm_model.predict(padded)\n    \n    # RF prediction\n    rf_probs = rf_model.predict_proba(np.mean(padded, axis=1))\n    rf_probs = np.array([np.pad(p, (0, len(emotion_labels) - len(p))) for p in rf_probs])\n    \n    # Hybrid fusion\n    hybrid_probs = 0.6*lstm_probs + 0.4*rf_probs\n    \n    # Perceptual smoothing\n    smoothed = np.convolve(np.mean(hybrid_probs, axis=0), np.ones(3)/3, mode='same')\n    \n    # Plot results\n    plt.figure(figsize=(8, 5))\n    plt.bar(emotion_labels, smoothed)\n    plt.title(f\"Predicted Emotions for {f}\")\n    plt.ylabel(\"Probability\")\n    plt.ylim(0,1)\n    plt.show()\n    \n    # Print prediction\n    pred_emotion = emotion_labels[np.argmax(smoothed)]\n    print(f\"Audio: {f} ‚Üí Predicted Emotion: {pred_emotion}\")\n    print(\"Probability per emotion:\")\n    for label, prob in zip(emotion_labels, smoothed):\n        print(f\"  {label}: {prob:.3f}\")\n    print(\"\\n\" + \"-\"*40 + \"\\n\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}