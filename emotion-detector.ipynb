{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13477706,"sourceType":"datasetVersion","datasetId":8556482}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Cell 1: Setup and Installation","metadata":{}},{"cell_type":"code","source":"!pip install librosa scikit-learn matplotlib numpy pandas noisereduce tensorflow shap","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"execution_failed":"2025-10-23T17:47:36.568Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell 1.5","metadata":{}},{"cell_type":"code","source":"# Cell 1.5: Updated - No unzip needed since files are already extracted\nimport os\nimport shutil\n\n# Paths to uploaded files\ndataset_path = '/kaggle/input/train-audio'\ndata_dir = '/kaggle/working/violin-emotion-analysis/data'\ncsv_path = '/kaggle/input/train-audio/emotion_labels.csv'\n\n# Create the working directory\nos.makedirs(data_dir, exist_ok=True)\n\n# Copy all audio files from the dataset to our working directory\naudio_source_dir = '/kaggle/input/train-audio/audio'\nif os.path.exists(audio_source_dir):\n    # Copy all WAV files to our working directory\n    for file_name in os.listdir(audio_source_dir):\n        if file_name.endswith('.wav'):\n            source_path = os.path.join(audio_source_dir, file_name)\n            dest_path = os.path.join(data_dir, file_name)\n            shutil.copy2(source_path, dest_path)\n    \n    print(f\"Copied audio files to working directory\")\nelse:\n    print(\"Audio folder not found!\")\n\n# Check folder structure\nprint(\"\\nData directory structure:\")\nfor root, dirs, files in os.walk(data_dir):\n    level = root.replace(data_dir, '').count(os.sep)\n    indent = ' ' * 2 * level\n    print(f\"{indent}{os.path.basename(root)}/\")\n    subindent = ' ' * 2 * (level + 1)\n    for f in files:\n        print(f\"{subindent}{f}\")\n\n# Check if CSV exists\nif not os.path.exists(csv_path):\n    print(\"\\nWARNING: 'emotion_labels.csv' not found!\")\nelse:\n    print(f\"\\nCSV file found: {csv_path}\")\n    # Also copy CSV to working directory for consistency\n    shutil.copy2(csv_path, '/kaggle/working/violin-emotion-analysis/emotion_labels.csv')\n    print(\"CSV copied to working directory\")\n\nprint(f\"\\nTotal audio files ready for processing: {len([f for f in os.listdir(data_dir) if f.endswith('.wav')])}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell 2: Imports and Updated Utility Functions","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport librosa\nimport librosa.display\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom sklearn.ensemble import RandomForestClassifier\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization, Attention, Input\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nimport noisereduce as nr\nimport shap\nimport joblib\nimport scipy.signal as signal\nfrom tqdm import tqdm\nimport warnings\nimport soundfile as sf\nwarnings.filterwarnings('ignore')\n\n# Create directories\nos.makedirs('/kaggle/working/violin-emotion-analysis/data', exist_ok=True)\nos.makedirs('/kaggle/working/violin-emotion-analysis/models', exist_ok=True)\n\n# ===============================================================\n# 1Ô∏è‚É£ Feature Extraction Helper (Temporal + Cleaned Audio)\n# ===============================================================\n\ndef extract_temporal_features(file_path, sr=44100, frame_length=2048, hop_length=512, window_size=3.0, hop_time=1.5):\n    \"\"\"\n    Extract richer emotion-aware features from violin recordings.\n    Captures pitch, tonality, articulation, dynamics, dissonance, and temporal evolution.\n    \"\"\"\n    y, sr = librosa.load(file_path, sr=sr)\n    y = nr.reduce_noise(y=y, sr=sr)\n\n    total_duration = librosa.get_duration(y=y, sr=sr)\n    step = int(hop_time * sr)\n    window = int(window_size * sr)\n\n    feature_sequences = []\n    for start in range(0, len(y) - window, step):\n        segment = y[start:start + window]\n        segment_features = []\n\n        # === Core tone features ===\n        mfccs = librosa.feature.mfcc(y=segment, sr=sr, n_mfcc=13)\n        mfccs_delta = librosa.feature.delta(mfccs)\n        chroma = librosa.feature.chroma_stft(y=segment, sr=sr)\n        chroma_delta = librosa.feature.delta(chroma)\n        spectral_contrast = librosa.feature.spectral_contrast(y=segment, sr=sr)\n\n        # === Expression features (temporal dynamics) ===\n        rms = librosa.feature.rms(y=segment)  # energy dynamics\n        zcr = librosa.feature.zero_crossing_rate(y=segment)  # articulation\n        centroid = librosa.feature.spectral_centroid(y=segment, sr=sr)\n        bandwidth = librosa.feature.spectral_bandwidth(y=segment, sr=sr)\n        flatness = librosa.feature.spectral_flatness(y=segment)\n        tempo, _ = librosa.beat.beat_track(y=segment, sr=sr)\n        harmony = librosa.feature.tonnetz(y=librosa.effects.harmonic(segment), sr=sr)\n\n        # === Statistical + temporal summarization per segment ===\n        # For RMS and ZCR, include their mean, std, and curve (downsampled)\n        curve_len = 20  # downsample curve to 20 points\n        rms_curve = np.interp(np.linspace(0, rms.shape[1]-1, curve_len), np.arange(rms.shape[1]), rms[0])\n        zcr_curve = np.interp(np.linspace(0, zcr.shape[1]-1, curve_len), np.arange(zcr.shape[1]), zcr[0])\n\n        feature_vec = np.hstack([\n            np.mean(mfccs, axis=1), np.std(mfccs, axis=1),\n            np.mean(mfccs_delta, axis=1), np.std(mfccs_delta, axis=1),\n            np.mean(chroma, axis=1), np.mean(chroma_delta, axis=1),\n            np.mean(spectral_contrast, axis=1),\n            np.mean(centroid), np.std(centroid),\n            np.mean(bandwidth),\n            np.mean(flatness),\n            tempo if not np.isnan(tempo) else 0,\n            np.mean(harmony, axis=1),\n            np.mean(rms), np.std(rms), rms_curve,  # include RMS curve\n            np.mean(zcr), np.std(zcr), zcr_curve   # include ZCR curve\n        ])\n\n        feature_sequences.append(feature_vec)\n\n    return np.array(feature_sequences)\n\n# ===============================================================\n# 2Ô∏è‚É£ Data Augmentation Helper\n# ===============================================================\ndef augment_audio(y, sr):\n    \"\"\"\n    Create slightly altered versions of the same clip to make the model more robust.\n    Includes small pitch shifts and speed changes.\n    \"\"\"\n    augmented = []\n\n    # Pitch shifts: ¬±2 semitones\n    for n_steps in [-2, 2]:\n        augmented.append(librosa.effects.pitch_shift(y, sr=sr, n_steps=n_steps))\n\n    # Time stretching using resampling (since librosa.effects.time_stretch expects a spectrogram now)\n    for rate in [0.9, 1.1]:\n        new_length = int(len(y) / rate)\n        augmented.append(librosa.resample(y, orig_sr=sr, target_sr=int(sr * rate))[:new_length])\n\n    return augmented\n\n\n# ===============================================================\n# 3Ô∏è‚É£ Create Temporal Dataset (with Augmented Samples)\n# ===============================================================\ndef create_temporal_dataset(data_dir, emotion_csv_path, augment=True):\n    \"\"\"\n    Creates dataset with temporal features and soft emotion labels.\n    The CSV must include: filename, emotion, and soft label columns (e.g., happy, sad, angry).\n    \"\"\"\n    annotations = pd.read_csv(emotion_csv_path)\n    X, y_soft = [], []\n\n    for _, row in tqdm(annotations.iterrows(), total=len(annotations), desc=\"Creating dataset\"):\n        file_path = os.path.join(data_dir, row['filename'])\n        if not os.path.exists(file_path):\n            continue\n\n        # Extract base features\n        base_features = extract_temporal_features(file_path)\n        X.append(base_features)\n        y_soft.append(row[4:].values.astype(float))  # ‚Üê soft emotion probabilities\n        \n        # --- Optional Augmentation ---\n        if augment:\n            y, sr = librosa.load(file_path, sr=44100)\n            y = nr.reduce_noise(y=y, sr=sr)\n            y = librosa.util.normalize(y)\n            for aug_y in augment_audio(y, sr):\n                # Save augmented version's features\n                temp_file = \"augmented_temp.wav\"\n                sf.write(temp_file, aug_y, sr)\n                aug_features = extract_temporal_features(temp_file)\n                X.append(aug_features)\n                y_soft.append(row[4:].values.astype(float))\n                os.remove(temp_file)\n\n    # Pad sequences for LSTM\n    max_len = max(x.shape[0] for x in X)\n    num_features = X[0].shape[1]\n    X_padded = np.zeros((len(X), max_len, num_features))\n    for i, seq in enumerate(X):\n        X_padded[i, :seq.shape[0], :] = seq\n\n    return np.array(X_padded), np.array(y_soft)\n\nprint(\"‚úÖ Advanced preprocessing module loaded (with noise reduction, normalization, and augmentation).\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell 3: Data Preparation","metadata":{}},{"cell_type":"code","source":"data_dir = '/kaggle/working/violin-emotion-analysis/data'\nemotion_csv_path = '/kaggle/working/violin-emotion-analysis/emotion_labels.csv'  # <-- upload your CSV here\n\nif not os.path.exists(emotion_csv_path):\n    print(\"Please upload 'emotion_labels.csv' with soft emotion probabilities.\")\nelse:\n    X, y_soft = create_temporal_dataset(data_dir, emotion_csv_path)\n    print(f\"Loaded {X.shape[0]} samples with {X.shape[2]} features each and {y_soft.shape[1]} soft emotion dimensions.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell 4: Model Training (LSTM + Random Forest)","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, LSTM, Dense, Dropout, Bidirectional, LayerNormalization\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import r2_score\nimport joblib\nimport json\nimport numpy as np\nimport os\n\nclass HybridEmotionModel:\n    def __init__(self, lstm_model, rf_model, input_shape, output_dim):\n        self.lstm_model = lstm_model\n        self.rf_model = rf_model\n        self.input_shape = input_shape\n        self.output_dim = output_dim\n\n    @staticmethod\n    def build_lstm(input_shape, output_dim):\n        inp = Input(shape=input_shape)\n        x = Bidirectional(LSTM(128, return_sequences=True))(inp)\n        x = LayerNormalization()(x)\n        x = Dropout(0.3)(x)\n        x = Bidirectional(LSTM(64))(x)\n        x = Dropout(0.3)(x)\n        lstm_output = Dense(64, activation='relu', name=\"lstm_output\")(x)\n        out = Dense(output_dim, activation='softmax')(lstm_output)\n        model = Model(inputs=inp, outputs=out)\n        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n                      loss='categorical_crossentropy',\n                      metrics=['accuracy'])\n        return model\n\n    def predict(self, X):\n        lstm_preds = self.lstm_model.predict(X)\n        embedding_model = Model(inputs=self.lstm_model.input,\n                                outputs=self.lstm_model.get_layer(\"lstm_output\").output)\n        embeddings = embedding_model.predict(X)\n        rf_preds = self.rf_model.predict(embeddings)\n        fused = 0.7 * lstm_preds + 0.3 * rf_preds\n        return fused\n\n    def save(self, folder_path='hybrid_model'):\n        os.makedirs(folder_path, exist_ok=True)\n        # Save LSTM\n        self.lstm_model.save(os.path.join(folder_path, 'lstm_model.h5'))\n        # Save RF\n        joblib.dump(self.rf_model, os.path.join(folder_path, 'rf_model.pkl'))\n        # Save metadata\n        meta = {\n            'input_shape': self.input_shape,\n            'output_dim': self.output_dim\n        }\n        with open(os.path.join(folder_path, 'meta.json'), 'w') as f:\n            json.dump(meta, f)\n        print(f\"‚úÖ Hybrid model saved to '{folder_path}'\")\n\n    @classmethod\n    def load(cls, folder_path='hybrid_model'):\n        lstm_model = tf.keras.models.load_model(os.path.join(folder_path, 'lstm_model.h5'))\n        rf_model = joblib.load(os.path.join(folder_path, 'rf_model.pkl'))\n        with open(os.path.join(folder_path, 'meta.json'), 'r') as f:\n            meta = json.load(f)\n        return cls(lstm_model, rf_model, tuple(meta['input_shape']), meta['output_dim'])\n\n\n# --- Prepare data ---\nX_train, X_test, y_train, y_test = train_test_split(X, y_soft, test_size=0.2, random_state=42)\ninput_shape = (X_train.shape[1], X_train.shape[2])\noutput_dim = y_train.shape[1]\n\n# --- Train multiple runs and pick best ---\nbest_r2 = -np.inf\nbest_hybrid = None\n\nfor run_seed in range(3):  # run 3 times, pick best\n    print(f\"\\nüîπ Training run {run_seed+1}\")\n    np.random.seed(run_seed)\n    tf.random.set_seed(run_seed)\n    \n    lstm_model = HybridEmotionModel.build_lstm(input_shape, output_dim)\n    early_stop = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n    \n    lstm_model.fit(\n        X_train, y_train,\n        validation_split=0.2,\n        epochs=80,\n        batch_size=16,\n        callbacks=[early_stop],\n        verbose=1\n    )\n    \n    embedding_model = Model(inputs=lstm_model.input,\n                            outputs=lstm_model.get_layer(\"lstm_output\").output)\n    train_embeddings = embedding_model.predict(X_train)\n    test_embeddings = embedding_model.predict(X_test)\n    \n    rf_model = RandomForestRegressor(n_estimators=200, max_depth=10, random_state=run_seed)\n    rf_model.fit(train_embeddings, y_train)\n    \n    # Weighted fusion\n    lstm_preds = lstm_model.predict(X_test)\n    rf_preds = rf_model.predict(test_embeddings)\n    fused_preds = 0.7 * lstm_preds + 0.3 * rf_preds\n    \n    r2 = r2_score(y_test, fused_preds)\n    print(f\"Run {run_seed+1} Hybrid R¬≤ Score: {r2:.4f}\")\n    \n    if r2 > best_r2:\n        best_r2 = r2\n        best_hybrid = HybridEmotionModel(lstm_model, rf_model, input_shape, output_dim)\n\n# --- Save the best hybrid ---\nbest_hybrid.save('hybrid_model')\nprint(f\"üèÜ Best hybrid model R¬≤: {best_r2:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell 5: Explainability and Visualization","metadata":{}},{"cell_type":"code","source":"# Cell 5: Enhanced SHAP Analysis with Hybrid Model Embeddings\nprint(\"üéµ SHAP Analysis - Feature Importance by Emotion\")\n\nimport shap\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# --- Define emotion labels ---\nemotion_labels = ['Happy', 'Sad', 'Angry']  # Update if needed\n\n# --- Get embeddings for SHAP ---\nembedding_model = Model(inputs=best_hybrid.lstm_model.input,\n                        outputs=best_hybrid.lstm_model.get_layer(\"lstm_output\").output)\n\ntrain_embeddings = embedding_model.predict(X_train[:100])  # First 100 training samples\ntest_embeddings = embedding_model.predict(X_test[:50])     # First 50 test samples\n\n# --- Create SHAP explainer ---\nexplainer = shap.Explainer(best_hybrid.rf_model, train_embeddings)\nshap_values = explainer(test_embeddings)\n\nprint(\"üîç Analyzing feature importance for each emotion...\")\n\n# --- Feature names (one per embedding neuron) ---\nnum_embeddings = train_embeddings.shape[1]\nfeature_categories = [f\"LSTM_Embedding_{i+1}\" for i in range(num_embeddings)]\n\n# --- Analyze SHAP values per emotion ---\nfor emotion_idx, emotion_name in enumerate(emotion_labels):\n    print(f\"\\nüéª {emotion_name.upper()} EMOTION:\")\n    print(\"=\"*40)\n    \n    # Get SHAP values for this specific emotion\n    emotion_shap_values = shap_values.values[:, :, emotion_idx]\n    \n    # Top 10 important embedding neurons\n    feature_importance = np.mean(np.abs(emotion_shap_values), axis=0)\n    top_feature_indices = np.argsort(feature_importance)[-10:][::-1]\n    \n    print(f\"Top embedding neurons predicting {emotion_name}:\")\n    for i, idx in enumerate(top_feature_indices):\n        importance = feature_importance[idx]\n        print(f\"  {i+1}. {feature_categories[idx]}: {importance:.4f}\")\n\n# --- SHAP summary plot per emotion ---\nfor emotion_idx, emotion_name in enumerate(emotion_labels):\n    print(f\"\\nüìà Generating SHAP summary plot for {emotion_name} embeddings...\")\n    shap.summary_plot(\n        shap_values.values[:, :, emotion_idx],   # Only this emotion\n        test_embeddings,\n        feature_names=feature_categories,\n        show=False\n    )\n    plt.title(f\"SHAP Feature Importance - {emotion_name}\")\n    plt.tight_layout()\n    plt.show()\n\n# --- Optional: Detailed explanation for first test sample ---\nprint(\"\\nüî¨ Detailed prediction breakdown for first test sample:\")\nsample_idx = 0\nshap.force_plot(\n    explainer.expected_value[0], \n    shap_values.values[sample_idx, :, 0], \n    test_embeddings[sample_idx, :],\n    feature_names=feature_categories,\n    matplotlib=True,\n    show=False\n)\nplt.title(f\"Prediction Breakdown: {emotion_labels[np.argmax(y_test[sample_idx])]} ‚Üí {emotion_labels[np.argmax(best_hybrid.predict(X_test[sample_idx:sample_idx+1]))]}\")\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}